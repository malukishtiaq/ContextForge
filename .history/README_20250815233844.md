# ContextForge - RAG Agent System

A production-ready Retrieval-Augmented Generation (RAG) system that ingests PDF documents, processes them into searchable chunks, and provides intelligent question-answering with page-level citations.

## ğŸš€ Features

- **PDF Processing**: Handles PDFs from small to 1000+ pages
- **Intelligent Chunking**: Sentence-aware text chunking with configurable overlap
- **Vector Search**: Qdrant-based vector storage with OpenAI embeddings
- **Smart Retrieval**: Advanced document retrieval with deduplication and confidence scoring
- **Citation Support**: Page-level citations for all answers
- **Background Processing**: Redis-based job queue for large document ingestion
- **RESTful API**: FastAPI-based API ready for web/mobile clients
- **Modular Architecture**: Pluggable interfaces for easy customization

## ğŸ—ï¸ Architecture

```
ContextForge/
â”œâ”€â”€ app/                    # FastAPI application
â”‚   â”œâ”€â”€ api/               # API routes and schemas
â”‚   â”œâ”€â”€ core/              # Configuration and utilities
â”‚   â”œâ”€â”€ db/                # Database models and repository
â”‚   â”œâ”€â”€ services/          # Core RAG services
â”‚   â”‚   â”œâ”€â”€ parser/        # PDF parsing (PyMuPDF)
â”‚   â”‚   â”œâ”€â”€ chunker/       # Text chunking
â”‚   â”‚   â”œâ”€â”€ embeddings/    # OpenAI embeddings
â”‚   â”‚   â”œâ”€â”€ vectorstore/   # Qdrant vector storage
â”‚   â”‚   â”œâ”€â”€ retriever/     # Document retrieval
â”‚   â”‚   â”œâ”€â”€ reranker/      # Optional LLM reranking
â”‚   â”‚   â””â”€â”€ answerer/      # Response generation
â”‚   â””â”€â”€ workers/           # Background job processing
â”œâ”€â”€ data/                  # Document storage and metadata
â”œâ”€â”€ tests/                 # Test suite
â””â”€â”€ docker-compose.yml     # Infrastructure setup
```

## ğŸ› ï¸ Tech Stack

- **Backend**: Python 3.11+, FastAPI, Uvicorn
- **Database**: SQLite (metadata), Qdrant (vectors)
- **Queue**: Redis + RQ
- **Embeddings**: OpenAI text-embedding-3-small
- **LLM**: OpenAI GPT models
- **PDF Processing**: PyMuPDF (fitz)
- **Containerization**: Docker + Docker Compose

## ğŸ“‹ Prerequisites

- Python 3.11+
- Docker and Docker Compose
- OpenAI API key
- Git

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
git clone <your-repo-url>
cd ragagent_understanding
```

### 2. Environment Configuration

```bash
cd ContextForge
cp .env.sample .env
# Edit .env with your OpenAI API key and other settings
```

### 3. Start Infrastructure

```bash
docker-compose up -d
```

This starts:
- Qdrant (vector database) on port 6333
- Redis (job queue) on port 6379

### 4. Install Dependencies

```bash
pip install -r requirements.txt
```

### 5. Run the Application

```bash
# Terminal 1: Start the API server
make run

# Terminal 2: Start the background worker
make worker
```

The API will be available at `http://localhost:8080`

### 6. API Documentation

- Swagger UI: `http://localhost:8080/docs`
- ReDoc: `http://localhost:8080/redoc`

## ğŸ“š Usage

### Upload a Document

```bash
curl -X POST "http://localhost:8080/v1/documents" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your_document.pdf"
```

### Ask a Question

```bash
curl -X POST "http://localhost:8080/v1/answers" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is the main topic of this document?",
    "docIds": ["your-document-id"],
    "topK": 10
  }'
```

## ğŸ”§ Configuration

Key environment variables in `.env`:

```bash
# OpenAI
OPENAI_API_KEY=your_api_key_here
EMBEDDING_MODEL=text-embedding-3-small
CHAT_MODEL=gpt-4o-mini

# Processing
CHUNK_TARGET_TOKENS=800
CHUNK_OVERLAP_TOKENS=100
MAX_CONTEXT_TOKENS=2000

# Features
ENABLE_RERANK=false
ENABLE_OCR=false
```

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run specific test
pytest tests/test_chunker.py -v
```

## ğŸ“ Project Structure

- **`app/api/`**: REST API endpoints
- **`app/services/`**: Core RAG services
- **`app/workers/`**: Background job processing
- **`app/db/`**: Database models and operations
- **`data/`**: Document storage and metadata database

## ğŸ”„ Workflow

1. **Document Upload**: PDF is uploaded via API
2. **Processing**: Background worker parses, chunks, and embeds the document
3. **Storage**: Chunks are stored in Qdrant with metadata
4. **Query**: User asks a question
5. **Retrieval**: System finds relevant chunks using vector similarity
6. **Answer**: LLM generates response with citations
7. **Response**: Answer returned with confidence metrics

## ğŸš§ Development

### Code Quality

```bash
# Format code
make fmt

# Lint code
make lint
```

### Adding New Features

- Follow the existing service pattern
- Implement interfaces for pluggable components
- Add comprehensive tests
- Update documentation

## ğŸ“Š Performance

- **Chunking**: ~800 tokens per chunk with 100 token overlap
- **Embedding**: Batch processing for efficiency
- **Search**: Configurable top-K retrieval with deduplication
- **Response**: Context-aware answer generation

## ğŸ”’ Security

- CORS enabled for web clients
- Input validation on all endpoints
- Structured error handling
- No sensitive data in logs

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

[Add your license here]

## ğŸ†˜ Support

For issues and questions:
- Check the documentation
- Review existing issues
- Create a new issue with detailed information

## ğŸ”® Roadmap

- [ ] Multi-modal document support
- [ ] Advanced OCR integration
- [ ] Conversation memory
- [ ] Document versioning
- [ ] Export capabilities
- [ ] Authentication system
- [ ] Performance monitoring
- [ ] Multi-language support

---

**Built with â¤ï¸ for intelligent document understanding**
